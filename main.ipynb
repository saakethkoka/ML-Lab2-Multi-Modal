{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 18:47:24.842282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 18:47:25.849377: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.4.4-ctldo35wmmwws3jbgwkgjjcjawddu3qz/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-02-27 18:47:25.849448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.4.4-ctldo35wmmwws3jbgwkgjjcjawddu3qz/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-02-27 18:47:25.849455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from base64 import b64decode\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "from IPython.display import display\n",
    "import gc\n",
    "import pickle\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# eager execution\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/work/users/skoka/Data/WikipediaImageCaptionsProccessed/'\n",
    "PICKLE_DIRECTORY = \"/work/users/skoka/Data/WikipediaImageCaptionsPickles/\"\n",
    "TARGET_LANGUAGES = ['en', 'es', 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists all .json files in the data folder\n",
    "files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.json')]\n",
    "files.sort()\n",
    "pickles = [f for f in os.listdir(PICKLE_DIRECTORY + \"processed_data\") if f.endswith('.pickle')]\n",
    "pickles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each json file has multiple json documents so we need to split them before loading\n",
    "def load_file_into_json(file):\n",
    "    with open(file) as f:\n",
    "        json_list = [json.loads(line) for line in f.read().splitlines()]\n",
    "    return json_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data into the format shown below:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"image\": \"numpy array of image\", \n",
    "        \"captions\": {\n",
    "            \"en\" : \"the caption\",\n",
    "            \"es\" : \"el caption\",\n",
    "            ....\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "This code will then dump all the data into pickle files for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _image_bytes_to_numpy(image_bytes):\n",
    "    image_decoded = b64decode(image_bytes)\n",
    "    image = Image.open(BytesIO(image_decoded)).convert(\"RGB\") # Opens image from the decoded bytes\n",
    "    image = image.resize((250, 250)) # Resizes all images to 250x250 to remain consistent\n",
    "    image = np.array(image) # Converts image to numpy array\n",
    "    return image\n",
    "\n",
    "# Gets the captions for the targeted languages if they exist\n",
    "def _process_captions(captions):\n",
    "    processed_captions = {}\n",
    "    for caption in captions:\n",
    "        if caption['language'] in TARGET_LANGUAGES:\n",
    "            # Sometimes the caption is non existent so we need to catch the error\n",
    "            try:\n",
    "                processed_captions[caption['language']] = caption['caption_reference_description']\n",
    "            except:\n",
    "                continue\n",
    "    return processed_captions\n",
    "\n",
    "def process_data(json_list):\n",
    "    data = []\n",
    "    for json_doc in json_list:\n",
    "        image = _image_bytes_to_numpy(json_doc['b64_bytes'])\n",
    "        captions = _process_captions(json_doc['wit_features'])\n",
    "        curr_data = {\n",
    "            'image': image,\n",
    "            'captions': captions\n",
    "        }\n",
    "        data.append(curr_data)\n",
    "    return data\n",
    "def load_data_from_pickle(pickle_file):\n",
    "    with open(PICKLE_DIRECTORY + \"processed_data/\" + pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data to use in English only caption generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "with open(PICKLE_DIRECTORY + \"captions.pickle\", \"rb\") as f:\n",
    "    captions = pickle.load(f)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index['<STAAART>'] = vocab_size\n",
    "tokenizer.word_index['<ENDDD>'] = vocab_size + 1\n",
    "vocab_size += 2\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 516783 word vectors.\n",
      "Embedding Shape: (593724, 300) \n",
      " Total words found: 117451 \n",
      " Percentage: 19.782087299822813\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(filename, embed_size):\n",
    "    # the embed size should match the file you load glove from\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    # save key/array pairs of the embeddings\n",
    "    #  the key of the dictionary is the word, the array is the embedding\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # now fill in the matrix, using the ordering from the\n",
    "    #  keras word tokenizer from before\n",
    "    found_words = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be ALL-ZEROS\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found_words = found_words+1\n",
    "\n",
    "    print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "        \"Total words found:\",found_words, \"\\n\",\n",
    "        \"Percentage:\",100*found_words/embedding_matrix.shape[0])\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_embeddings(\"/users/skoka/Documents/ML-Lab2-Multi-Modal/numberbatch-en-19.08.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CAPTION_LENGTH = 25\n",
    "# Create a data generator to load the data in batches to avoid memory issues\n",
    "# Only for English captions\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    # remove non alphanumeric characters\n",
    "    caption = re.sub(r'[^a-zA-Z0-9\\s]', '', caption)\n",
    "    # add start and end tokens\n",
    "    caption = '<STAAART> ' + caption + ' <ENDDD>'\n",
    "    return caption\n",
    "\n",
    "def data_generator(batch_size=32):\n",
    "    X1, X2, y = [], [], []\n",
    "    n = 0\n",
    "    while True:\n",
    "        for pickle_file in pickles:\n",
    "            data = load_data_from_pickle(pickle_file)\n",
    "            for doc in data:\n",
    "                image = doc['image']\n",
    "                captions = doc['captions']\n",
    "                if 'en' in captions:\n",
    "                    caption = captions['en']\n",
    "                    caption = clean_caption(caption)\n",
    "                    caption = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    for i in range(1, len(caption)):\n",
    "                        in_seq, out_seq = caption[:i], caption[i]\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=MAX_CAPTION_LENGTH)[0]\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        X1.append(image)\n",
    "                        X2.append(in_seq)\n",
    "                        y.append(out_seq)\n",
    "                        n += 1\n",
    "                        if n == batch_size:\n",
    "                            yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                            X1, X2, y = [], [], []\n",
    "                            n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 18:52:30.403614: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 18:52:34.457130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79117 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.461039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79117 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.470604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79117 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.476358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79117 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.479952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 79117 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.483547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 79117 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.487167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 79117 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2023-02-27 18:52:34.496065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 79117 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 8\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 250, 250, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 250, 250, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 250, 250, 64  36928       ['block1_conv1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 125, 125, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 125, 125, 12  73856       ['block1_pool[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 125, 125, 12  147584      ['block2_conv1[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 62, 62, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 62, 62, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 62, 62, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 62, 62, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv4 (Conv2D)          (None, 62, 62, 256)  590080      ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 31, 31, 256)  0           ['block3_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 31, 31, 512)  1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 31, 31, 512)  2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 31, 31, 512)  2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv4 (Conv2D)          (None, 31, 31, 512)  2359808     ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 15, 15, 512)  0           ['block4_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 15, 15, 512)  2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 15, 15, 512)  2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 15, 15, 512)  2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv4 (Conv2D)          (None, 15, 15, 512)  2359808     ['block5_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 25)]         0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 115200)       0           ['block5_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 25, 300)      178117200   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          29491456    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          570368      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 593724)       152587068   ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 380,856,268\n",
      "Trainable params: 182,714,684\n",
      "Non-trainable params: 198,141,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # Build a VGG model that takes input of size 250x250x3\n",
    "    vgg = tf.keras.applications.VGG19(input_shape=(250, 250, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    # remove the last layer of the VGG model\n",
    "    vgg = tf.keras.Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n",
    "    vgg.trainable = False\n",
    "\n",
    "    image_dense = tf.keras.layers.Flatten()(vgg.output)\n",
    "    image_dense = tf.keras.layers.Dense(256, activation='relu')(image_dense)\n",
    "\n",
    "    # Build a LSTM model that takes input of size 25\n",
    "    caption_input = tf.keras.layers.Input(shape=(MAX_CAPTION_LENGTH,))\n",
    "    # caption_embedding = tf.keras.layers.Embedding(vocab_size, 50, mask_zero=True)(caption_input)\n",
    "    caption_embedding = tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False, input_length=MAX_CAPTION_LENGTH)(caption_input)\n",
    "    caption_lstm = tf.keras.layers.LSTM(256)(caption_embedding)\n",
    "\n",
    "\n",
    "    # Merge the two models\n",
    "    decoder1 = tf.keras.layers.add([image_dense, caption_lstm])\n",
    "    decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[vgg.input, caption_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # plot model:\n",
    "    model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1082989/4048201249.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch=1000, verbose=1)\n",
      "2023-02-27 19:36:17.577110: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:129\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 232s 232ms/step - loss: 11.2439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f0028ae50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = data_generator(batch_size=128)\n",
    "model.fit_generator(generator, epochs=1, steps_per_epoch=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image):\n",
    "    in_text = '<STAAART>'\n",
    "    for i in range(MAX_CAPTION_LENGTH):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=MAX_CAPTION_LENGTH)\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == '<ENDDD>':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 19:45:42.618958: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619003: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619026: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619068: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619167: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619205: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-02-27 19:45:42.619278: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1564 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(1662): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node model_1/dense_2/Softmax/_315}}]] [Op:__inference_predict_function_78447]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     [image, seq], _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(generator)\n\u001b[0;32m----> 4\u001b[0m     yhat \u001b[39m=\u001b[39m predict_caption(model, image)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(yhat)\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mpredict_caption\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m      4\u001b[0m sequence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences([in_text])[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m sequence \u001b[39m=\u001b[39m pad_sequences([sequence], maxlen\u001b[39m=\u001b[39mMAX_CAPTION_LENGTH)\n\u001b[0;32m----> 6\u001b[0m yhat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([image, sequence], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m yhat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(yhat)\n\u001b[1;32m      8\u001b[0m word \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mindex_word[yhat]\n",
      "File \u001b[0;32m~/.venv/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.venv/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node model_1/dense_2/Softmax/_315}}]] [Op:__inference_predict_function_78447]"
     ]
    }
   ],
   "source": [
    "generator = data_generator(batch_size=1)\n",
    "for i in range(10):\n",
    "    [image, seq], _ = next(generator)\n",
    "    yhat = predict_caption(model, image)\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
