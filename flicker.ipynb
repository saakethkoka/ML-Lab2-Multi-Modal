{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 20:44:14.687781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 20:44:15.757946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.4.4-ctldo35wmmwws3jbgwkgjjcjawddu3qz/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-03-02 20:44:15.758030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cudnn-8.2.4.15-11.4-eluwegpwn6adr7hlku5p5wru5xzefpop/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/cuda-11.4.4-ctldo35wmmwws3jbgwkgjjcjawddu3qz/lib64:/hpc/mp/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-10.3.0/neovim-0.7.0-terkir3wk5rst6ktv4uxyaqjditacv5p/lib\n",
      "2023-03-02 20:44:15.758036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIRECTORY = \"/work/users/skoka/Data/flickr30k_images/flickr30k_images/\"\n",
    "LABELS_FILE = \"/work/users/skoka/Data/flickr30k_images/results.csv\"\n",
    "PICKLE_DIRECTORY = \"/work/users/skoka/Data/flicker30k_pickles/\"\n",
    "GET_FROM_PICKLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the labels\n",
    "labels = pd.read_csv(LABELS_FILE, delimiter='|')\n",
    "# convert labels[ ' comment' ] to strings\n",
    "labels[' comment'] = labels[' comment'].astype(str)\n",
    "\n",
    "# list of .jpg files from the directory\n",
    "image_files = [f for f in os.listdir(IMAGES_DIRECTORY) if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in a sentence:  13.389340213321587\n"
     ]
    }
   ],
   "source": [
    "# Average number of words in a sentence\n",
    "avg_words = 0\n",
    "for i in range(len(labels)):\n",
    "    avg_words += len(labels[' comment'][i].split())\n",
    "avg_words = avg_words / len(labels)\n",
    "print(\"Average number of words in a sentence: \", avg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a jpg file to a numpy array\n",
    "def _read_jpg(filename):\n",
    "    im = Image.open(filename)\n",
    "    # resize to 224x224\n",
    "    im = im.resize((224, 224))\n",
    "    return np.array(im)\n",
    "\n",
    "def process_files(files):\n",
    "    images = {\n",
    "        # \"filename_without_extension\" : numpy_array_of_image\n",
    "    }\n",
    "    num_read = 0\n",
    "    for f in files:\n",
    "        image_name = f.split('.')[0]\n",
    "        image = _read_jpg(IMAGES_DIRECTORY + f)\n",
    "        images[image_name] = {\n",
    "            \"image\": _read_jpg(IMAGES_DIRECTORY + f),\n",
    "            \"captions\": labels[labels.image_name == (image_name + \".jpg\")][\" comment\"].tolist()\n",
    "            }\n",
    "        num_read += 1\n",
    "        if num_read % 1000 == 0:\n",
    "            print(\"Read {} files\".format(num_read))\n",
    "    return images\n",
    "if not GET_FROM_PICKLE:\n",
    "    images = process_files(image_files)\n",
    "    with open(PICKLE_DIRECTORY + \"flicker_images.pkl\", \"wb\") as f:\n",
    "        pickle.dump(images, f)\n",
    "else:\n",
    "    with open(PICKLE_DIRECTORY + \"flicker_images.pkl\", \"rb\") as f:\n",
    "        images = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "{\n",
    "    \"image_name\" : {\n",
    "        \"image\" : \"image\", // Numpy array of the image\n",
    "        \"captions\" :  [\n",
    "            \"caption 1\",\n",
    "            \"caption 2\",\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "captions = labels[\" comment\"].to_numpy()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "tokenizer.fit_on_texts(['staaaart', 'endddd'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# tokenizer.word_index['staaaart'] = vocab_size -1\n",
    "# tokenizer.word_index['endddd'] = vocab_size\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18315"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "MAX_CAPTION_LENGTH = 20\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    # remove non alphanumeric characters\n",
    "    caption = re.sub(r'[^a-zA-Z0-9\\s]', '', caption)\n",
    "    # add start and end tokens\n",
    "    caption = 'staaaart ' + caption + ' endddd'\n",
    "    return caption\n",
    "total_time = 0\n",
    "def data_generator(batch_size=32):\n",
    "    global total_time\n",
    "    start = time.time()\n",
    "    X1, X2, y = [], [], []\n",
    "    n = 0\n",
    "    while True:\n",
    "        for image_name, image_data in images.items():\n",
    "            image = image_data[\"image\"]\n",
    "            captions = image_data[\"captions\"]\n",
    "            for caption in captions:\n",
    "                caption = clean_caption(caption)\n",
    "                caption = tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(caption)):\n",
    "                    in_seq, out_seq = caption[:i], caption[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=MAX_CAPTION_LENGTH)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    X1.append(image)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                    n += 1\n",
    "                    if n == batch_size:\n",
    "                        X1 = np.array(X1)\n",
    "                        X1 = X1 / 255\n",
    "                        X2 = np.array(X2)\n",
    "                        y = np.array(y)\n",
    "                        total_time += time.time() - start\n",
    "                        yield [X1, X2], y\n",
    "                        start = time.time()\n",
    "                        X1, X2, y = [], [], []\n",
    "                        n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, embed_size):\n",
    "    # the embed size should match the file you load glove from\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    # save key/array pairs of the embeddings\n",
    "    #  the key of the dictionary is the word, the array is the embedding\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # now fill in the matrix, using the ordering from the\n",
    "    #  keras word tokenizer from before\n",
    "    found_words = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be ALL-ZEROS\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found_words = found_words+1\n",
    "\n",
    "    print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "        \"Total words found:\",found_words, \"\\n\",\n",
    "        \"Percentage:\",100*found_words/embedding_matrix.shape[0])\n",
    "    return embedding_matrix\n",
    "\n",
    "# embedding_matrix = load_embeddings(\"/users/skoka/Documents/ML-Lab2-Multi-Modal/numberbatch-en-19.08.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv4 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv4 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv4 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 100352)       0           ['block5_conv4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          25690368    ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 256)       0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 20, 256)      4688896     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 21, 256)      0           ['reshape_2[0][0]',              \n",
      "                                                                  'embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 512)         1050624     ['concatenate_2[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 256)          131328      ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 18316)        4707212     ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56,292,812\n",
      "Trainable params: 36,268,428\n",
      "Non-trainable params: 20,024,384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a VGG model that takes input of size 250x250x3\n",
    "vgg = tf.keras.applications.VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# remove the last layer of the VGG model\n",
    "vgg = tf.keras.Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n",
    "vgg.trainable = False\n",
    "\n",
    "\n",
    "image_dense = tf.keras.layers.Flatten()(vgg.output)\n",
    "image_dense = tf.keras.layers.Dense(256, activation='relu')(image_dense)\n",
    "image_reshaped = tf.keras.layers.Reshape((1, 256))(image_dense)\n",
    "\n",
    "caption_input = tf.keras.layers.Input(shape=(MAX_CAPTION_LENGTH,))\n",
    "caption_embedding = tf.keras.layers.Embedding(vocab_size, 256, mask_zero=False)(caption_input)\n",
    "\n",
    "# Merge the two models\n",
    "decoder_add = tf.keras.layers.add([image_reshaped, caption_embedding], axis=1)\n",
    "\n",
    "# bi directonal LSTM\n",
    "decoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=False))(decoder_add)\n",
    "decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder_lstm)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[vgg.input, caption_input], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot model:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# plot model:\n",
    "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1000 [..............................] - ETA: 1:34 - loss: 3.8221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2874860/3685102400.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(data_generator(batch_size=64),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 39s 39ms/step - loss: 3.9230 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(data_generator(batch_size=64),\n",
    "                                epochs=1,\n",
    "                                steps_per_epoch=1000,\n",
    "                                callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.00000001, verbose=1)],\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image):\n",
    "    in_text = 'staaaart'\n",
    "    for i in range(MAX_CAPTION_LENGTH):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=MAX_CAPTION_LENGTH)\n",
    "        print(sequence)\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # print top 5 predicted words:\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endddd':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Caption:  A man in a white t-shirt does a trick with a bronze colored yo-yo .\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0 18314]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0 18314     1]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 18314     1     6]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0 18314     1     6     2]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0 18314     1     6     2     1]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 18314     1     6     2     1    24]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0 18314     1     6     2     1    24    18]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  18314     1     6     2     1    24    18     7]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0 18314\n",
      "      1     6     2     1    24    18     7    38]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0 18314     1\n",
      "      6     2     1    24    18     7    38    35]]\n",
      "[[    0     0     0     0     0     0     0     0     0 18314     1     6\n",
      "      2     1    24    18     7    38    35     1]]\n",
      "[[    0     0     0     0     0     0     0     0 18314     1     6     2\n",
      "      1    24    18     7    38    35     1    33]]\n",
      "Predicted Caption: staaaart a man in a blue shirt is walking down a street endddd\n"
     ]
    }
   ],
   "source": [
    "# Predict on a random image\n",
    "import random\n",
    "image_name = random.choice(list(images.keys()))\n",
    "image = images[image_name][\"image\"]\n",
    "caption = images[image_name][\"captions\"][0]\n",
    "print(\"Actual Caption:\", caption)\n",
    "predicted_caption = predict_caption(model, image.reshape(1, 224, 224, 3))\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
